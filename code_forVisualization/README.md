This code is related to the paper [What do different evaluation metrics tell us about saliency models?](http://arxiv.org/abs/1604.03605). It allows for the visualization of different metric computations, to add transparency to evaluation, explain metric behavior, and debug saliency models.

If you use any of this code, please cite: 

<pre>
@article{salMetrics_Bylinskii,
    title    = {What do different evaluation metrics tell us about saliency models?},
    author   = {Zoya Bylinskii and Tilke Judd and Aude Oliva and Antonio Torralba and Fr{\'e}do Durand},
    journal  = {arXiv preprint arXiv:1604.03605},
    year     = {2016}
}
</pre>

<pre>
@misc{mit-saliency-benchmark,
  author       = {Zoya Bylinskii and Tilke Judd and Fr{\'e}do Durand and Aude Oliva and Antonio Torralba},
  title        = {MIT Saliency Benchmark},
  howpublished = {http://saliency.mit.edu/}
}
</pre>

Please contact saliency@mit.edu with any questions.
 
